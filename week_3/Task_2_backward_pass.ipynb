{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Task 2\n\nImplement basic backward pass in MLP. Perform forward and backward propagation through your network and check your gradients.\nThis time, the forward pass is implemented for you. Notice the matrix notation - the dimensions are in form $[m,nX,1]$, where $m$ is batch size (number of samples) and $nX$ is the size of sample vector.",
      "metadata": {
        "cell_id": "00000-a52b8b38-8de1-48ae-af49-56868cea4bbd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00001-06170395-9010-4284-ad76-cd41fa70bce6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "fa935c01",
        "execution_millis": 0,
        "execution_start": 1615215999277,
        "deepnote_cell_type": "code"
      },
      "source": "# Import\nimport numpy as np",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Activations\n\nImplement derivations of standard activation functions (ReLU, Sigmoid), which are used in your task.",
      "metadata": {
        "cell_id": "00002-23573767-da5f-4504-9353-e637e7969f53",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00003-2ff9a244-d75f-453a-b6dc-e978920bd54a",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e1aa1c32",
        "execution_millis": 0,
        "execution_start": 1615216000846,
        "deepnote_cell_type": "code"
      },
      "source": "#------------------------------------------------------------------------------\n#   ActivationFunction class\n#------------------------------------------------------------------------------\nclass ActivationFunction:\n    def __init__(self):\n        pass\n\n    def __call__(self, z):\n        pass\n\n#------------------------------------------------------------------------------\n#   LinearActivationFunction class\n#------------------------------------------------------------------------------\nclass LinearActivationFunction(ActivationFunction):\n    def __call__(self, z):\n        return z\n\n    def derivation(self, z):\n        ###>>> start of solution\n        return np.ones_like(z)\n        ###<<< end of solution\n        pass\n\n#------------------------------------------------------------------------------\n#   RELUActivationFunction class\n#------------------------------------------------------------------------------\nclass RELUActivationFunction(ActivationFunction):\n    def __call__(self, z):\n        return np.maximum(z, 0)\n\n    def derivation(self, z):\n        ###>>> start of solution\n        return np.where(z > 0, 1., 0.)\n        ###<<< end of solution\n        pass\n\n#------------------------------------------------------------------------------\n#   SigmoidActivationFunction class\n#------------------------------------------------------------------------------\nclass SigmoidActivationFunction(ActivationFunction):\n    def __call__(self, z):\n        return self.sigmoid(z)\n\n    def sigmoid(self, x):\n        return 1.0/(1.0+np.exp(-x))\n\n    def derivation(self, z):\n        ###>>> start of solution\n        return self.sigmoid(z) * (1 - self.sigmoid(z))\n        ###<<< end of solution\n        pass\n    \n# Activation mapping\n    \nMAP_ACTIVATION_FUCTIONS = {\n    \"linear\": LinearActivationFunction,\n    \"relu\": RELUActivationFunction,\n    \"sigmoid\": SigmoidActivationFunction\n}\n\ndef CreateActivationFunction(kind):\n    if (kind in MAP_ACTIVATION_FUCTIONS):\n        return MAP_ACTIVATION_FUCTIONS[kind]()\n    raise ValueError(kind, \"Unknown activation function {0}\".format(kind))",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Layer\n\nThis is the main class which can hold different types of layers and provides us with standard tasks like forward propagation. Implement backward functions for defined classes.\n\nnUnits - number of neuron units in your layer\n\nprevLayer - previous layer (need it to know the shape of it to create appropriate number of weights for you to use in current layer)",
      "metadata": {
        "cell_id": "00004-4fbcf184-b4cb-4ba9-a4de-b0cee961e471",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "cell_id": "00005-8d6d2a92-1d4e-451f-aae3-a294168d2608",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "3ff34654",
        "execution_millis": 0,
        "execution_start": 1615216811147,
        "deepnote_cell_type": "code"
      },
      "source": "#------------------------------------------------------------------------------\n#   Layer class\n#------------------------------------------------------------------------------\nclass Layer:\n    def __init__(self, act=\"linear\", name=\"layer\"):\n        self.shape = (0, 0)\n        self.activation = CreateActivationFunction(act)\n        self.name = name\n\n    def initialize(self, prevLayer):\n        pass\n\n    def forward(self, x):\n        pass\n\n#------------------------------------------------------------------------------\n#   InputLayer class\n#------------------------------------------------------------------------------\nclass InputLayer(Layer):\n    def __init__(self, nUnits, name=\"Input\"):\n        super().__init__(act=\"linear\", name=name)\n        self.nUnits = nUnits\n\n    def initialize(self, prevLayer):\n        self.shape = (self.nUnits, 1)\n\n    def forward(self, x):\n        return x\n\n    def backward(self, X, *args):\n        return None\n    \n#------------------------------------------------------------------------------\n#   Basic Dense Layer class\n#------------------------------------------------------------------------------\nclass DenseLayer(Layer):\n    def __init__(self, nUnits, act=\"linear\", name=\"Dense\"):\n        super().__init__(act, name=name)\n        # init each neuron into list        \n        self.nUnits = nUnits\n        self.W = None\n        self.b = None\n\n    def initialize(self, prevLayer):\n        #initialize all neurons\n        self.shape = (self.nUnits, prevLayer.shape[0])\n\n        # Initialize weights and bias\n        prev_nUnits, _ = prevLayer.shape\n        self.W = np.random.randn(self.nUnits, prev_nUnits)\n        self.b = np.zeros((self.nUnits, 1), dtype=float)\n\n    def forward(self, X):\n        print(\"Forward of\", self.name)\n        self.z = np.matmul(self.W, X) + self.b         # Z = W*x + b\n        self.a = self.activation(self.z)               # a = activation(Z)\n        \n        return self.a\n\n    def backward(self, da, aPrev):\n        #   da  =   dLoss -> dL/da of previous layer - with respect to backward pass\n        #   aPrev   =   activation of previous layer needed for weights - with respect to forward pass\n        batch_size = aPrev.shape[0]\n        print(\"Backward of\", self.name)\n        ###>>> start of solution\n        dz = 0\n        dW = 0\n        db = 0\n        dz = self.activation.derivation(self.z) * da\n        for i in range(batch_size):\n            dW += np.sum(dz[i] * aPrev[i].T)\n        dW *= (1/batch_size)\n        db = np.sum(dz, axis=1, keepdims=True) * (1/ batch_size)\n        da_l = np.matmul(self.W.T, dz)\n        return np.sum(da_l) * (1/batch_size)\n        ###<<< end of solution\n        pass\n    ",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Loss Functions\n\nImplement two standard loss functions (Binary Cross Entropy and Mean Squared Error), which you will/can use in your implementation of MLP backward pass.",
      "metadata": {
        "cell_id": "00006-10eabb24-9432-4334-9932-db88115b89a6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00007-6b15e0bc-aae6-4ef7-bcd4-cb278a1ecac7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "af14bd89",
        "execution_millis": 0,
        "execution_start": 1615216002621,
        "deepnote_cell_type": "code"
      },
      "source": "#------------------------------------------------------------------------------\n#   LossFunction class\n#------------------------------------------------------------------------------\nclass LossFunction:\n    def __init__(self):\n        pass\n\n    def __call__(self, A, Y):\n        pass\n\n    def derivation(self, A, Y):\n        pass\n\n\n#------------------------------------------------------------------------------\n#   BinaryCrossEntropyLossFunction class\n#------------------------------------------------------------------------------\nclass BinaryCrossEntropyLossFunction(LossFunction):\n    def __call__(self, A, Y):\n        # Warning! Use of logarithm - Take care about definition scope\n        ###>>> start of solution\n        return -1/len(A) * np.sum(Y * np.log(A) + (1-Y) * (1-np.log(A)))\n        ###<<< end of solution\n        pass\n    \n    def derivation(self, A, Y):\n        # Warning! Use of logarithm - Take care about definition scope\n        ###>>> start of solution\n        return np.vectorize(lambda y, p: -1/p if y == 0 else 1/1-p)(Y, A)\n        ###<<< end of solution\n        pass\n    \nclass MeanSquaredErrorLossFunction(LossFunction):\n    def __call__(self, A, Y):\n        ###>>> start of solution\n        return np.sum((Y - A)**2) / len(A)\n        ###<<< end of solution\n        pass\n\n    def derivation(self, A, Y):\n        ###>>> start of solution\n        return np.vectorize(lambda y, p: -1 * (2 * (y - p)))(Y, A)\n        ###<<< end of solution\n        pass\n\n\nMAP_LOSS_FUNCTIONS = {\n    \"bce\": BinaryCrossEntropyLossFunction,\n    \"mse\": MeanSquaredErrorLossFunction\n}\n\ndef CreateLossFunction(kind):\n    if (kind in MAP_LOSS_FUNCTIONS):\n        return MAP_LOSS_FUNCTIONS[kind]()\n    raise ValueError(kind, \"Unknown loss function {0}\".format(kind))",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Model class\n\nThis is the basic class which holds all of your layers and encapsulate functionality to predict results from your input as a forward pass through all the layers after you create your model and initialize all the layers.\n\nImplemet backpropagation.",
      "metadata": {
        "cell_id": "00008-29252b07-6247-427f-9ee5-f7cb59a53f06",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "cell_id": "00009-d3a14d63-02ed-4f62-83fa-30cba666e4d8",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a862135a",
        "execution_millis": 0,
        "execution_start": 1615217039067,
        "deepnote_cell_type": "code"
      },
      "source": "#------------------------------------------------------------------------------\n#   Model class\n#------------------------------------------------------------------------------\nclass Model:\n    def __init__(self, lossName):\n        self.layers = []\n        # Initialize loss function\n        self.loss_fn = CreateLossFunction(lossName)\n        \n    def addLayer(self,  layer):\n        self.layers.append(layer)\n\n    def initialize(self):\n        # Call initialization sequentially on all layers\n        prevLayer = None\n        for l in self.layers:\n            l.initialize(prevLayer)\n            prevLayer = l      \n    \n    def forward(self, X):\n        # Single feed forward\n        A = X\n        for l in self.layers:\n            A = l.forward(A)\n            \n        return A\n    \n    def backward(self, dLoss):\n        ###>>> start of solution\n        da = dLoss\n        for layer, prev in zip(reversed(self.layers[:-1]), reversed(self.layers[1:])):\n            da = layer.backward(da, prev.a)\n        ###<<< end of solution\n        pass\n    \n    def compute_cost(self, A, Y):\n        batch_size = Y.shape[0]\n        \n        ###>>> start of solution\n        return np.sum(self.loss_fn(A, Y)) * (1/batch_size)\n        ###<<< end of solution\n        \n        pass\n    \n    def derive_loss(self, A, Y):\n        batch_size = Y.shape[0]\n\n        ###>>> start of solution\n        return np.sum(self.loss_fn.derivation(A, Y)) * (1/batch_size)\n        ###<<< end of solution\n        \n        pass",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Main Processing Cell\n\n 1. Initialize dataset. \n 2. Declare a simple model (at least 4 layer) with relu on hidden layers and sigmoid on output layer.\n 3. Perform forward pass through the network. \n 4. Compute loss.\n 5. Derive loss.\n 6. Perform backward pass.\n 7. Celebrate and scroll lower.",
      "metadata": {
        "cell_id": "00010-60a782f9-bb47-4e67-8b04-94a0f5dbe3a4",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00011-97a1b7d0-6355-4ba4-b69e-e06a71cf3dfe",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9f18f760",
        "execution_millis": 13,
        "execution_start": 1615217041487,
        "deepnote_cell_type": "code"
      },
      "source": "# Main processing\nfrom dataset import dataset_Circles\n# Task A:\n\nX, Y = dataset_Circles(n=16, radius=0.7, noise=0.0)\n###>>> start of solution\nm = Model(\"bce\")\nm.addLayer(InputLayer(2))\nm.addLayer(DenseLayer(12, act=\"relu\"))\nm.addLayer(DenseLayer(6, act=\"relu\"))\nm.addLayer(DenseLayer(3, act=\"relu\"))\nm.addLayer(DenseLayer(1, act=\"sigmoid\"))\nm.initialize()\n\nA = m.forward(X)\n\nloss = m.compute_cost(A, Y)\n\nprint(\"Loss = \",loss)\n\ndLoss = m.derive_loss(A, Y)\n\nback = m.backward(dLoss)\n###<<< end of solution",
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "text": "Forward of Dense\nForward of Dense\nForward of Dense\nForward of Dense\nLoss =  -0.03924397448882791\nBackward of Dense\nBackward of Dense\nBackward of Dense\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "**How does gradient checking work?**.\n\nAs in 1) and 2), you want to compare \"gradapprox\" to the gradient computed by backpropagation. The formula is still:\n\n$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n\nHowever, $\\theta$ is not a scalar anymore. It is a dictionary called \"parameters\". We implemented a function \"`dictionary_to_vector()`\" for you. It converts the \"parameters\" dictionary into a vector called \"values\", obtained by reshaping all parameters (W1, b1, W2, b2, W3, b3) into vectors and concatenating them.\n\nThe inverse function is \"`vector_to_dictionary`\" which outputs back the \"parameters\" dictionary.\n\n\nWe have also converted the \"gradients\" dictionary into a vector \"grad\" using gradients_to_vector(). You don't need to worry about that.\n\n\nHere is pseudo-code that will help you implement the gradient check.\n\nFor each i in num_parameters:\n- To compute `J_plus[i]`:\n    1. Set $\\theta^{+}$ to `np.copy(parameters_values)`\n    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n    3. Calculate $J^{+}_i$ using to `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n- To compute `J_minus[i]`: do the same thing with $\\theta^{-}$\n- Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n\nThus, you get a vector gradapprox, where gradapprox[i] is an approximation of the gradient with respect to `parameter_values[i]`. You can now compare this gradapprox vector to the gradients vector from backpropagation. Just like for the 1D case (Steps 1', 2', 3'), compute: \n$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\n\n\n**The code will be added later** but soon enough ;)",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "cell_id": "00012-131459c8-a273-4183-b99d-32a45be7c23b",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00013-26a24b4e-7313-4c9a-9a06-3096d579d0a3",
        "deepnote_cell_type": "code"
      },
      "source": "# GRADED FUNCTION: gradient_check_n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Verification cell\n\n 8. Verify your solution by gradient checking.\n 9. Start crying.\n 10. Repeat until correct ;)",
      "metadata": {
        "collapsed": false,
        "cell_id": "00014-4e216988-01c7-4945-b3bf-186c685e1798",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        },
        "cell_id": "00015-f19aeed2-95fe-41cd-985b-1987c306510f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8f002bfc",
        "execution_millis": 3,
        "execution_start": 1615216824009,
        "deepnote_cell_type": "code"
      },
      "source": "def gradient_check_n(network, X, Y, epsilon = 1e-7):\n    \"\"\"\n    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n\n    Arguments:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters.\n    x -- input datapoint, of shape (input size, 1)\n    y -- true \"label\"\n    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n\n    Returns:\n    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n    \"\"\"\n\n    # Set-up variables\n    gradapprox = []\n    grad_backward = []\n\n    for i,layer in enumerate(network.layers):\n        # Compute gradapprox\n        if i < 1:\n            continue\n        shape = layer.W.shape\n        # print(shape[0], ',', shape[1])\n        for i in range(shape[0]):\n            for j in range(shape[1]):\n                # print('i',i,'j',j)\n                # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n                # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n                origin_W = layer.W[i][j]\n\n                layer.W[i][j] = origin_W + epsilon\n                A_plus = network.forward(X)\n                J_plus = network.compute_loss(A_plus, Y)\n\n                # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n                layer.W[i][j] = origin_W - epsilon\n                A_minus = network.forward(X)\n                J_minus = network.compute_loss(A_minus, Y)\n\n                # Compute gradapprox[i]\n                gradapprox.append((J_plus - J_minus) / (2*epsilon))\n                # print(layer.name, layer.dW.shape)\n                # grad = np.mean(layer.dW, axis=0, keepdims=True)\n                # grad_backward.append(grad[0][i][j])\n                grad_backward.append(layer.dW[i][j])\n                layer.W[i][j] = origin_W\n\n\n    # Compare gradapprox to backward propagation gradients by computing difference.\n\n    gradapprox = np.reshape(gradapprox, (-1, 1))\n    grad_backward = np.reshape(grad_backward, (-1, 1))\n\n    numerator = np.linalg.norm(grad_backward - gradapprox)\n    denominator = np.linalg.norm(grad_backward) + np.linalg.norm(gradapprox)\n    difference = numerator / denominator\n\n    if difference > 2e-7:\n        print (\"\\033[91m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n    else:\n        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n\n",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=151ec30a-9db5-4600-8a2d-358dada1a208' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "deepnote_notebook_id": "cae34137-6825-4c44-8617-35ae5daa7ad4",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}