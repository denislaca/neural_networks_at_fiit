{"cells":[{"cell_type":"markdown","source":"# Task 1\n\nImplement basic layer which will consist of simple logical neurons. Allow your solution to stack multiple layers to form MLP network. Perform forward propagation through your network.\n\n","metadata":{"cell_id":"00000-58e8f2c6-bd5d-4bab-b71f-a01d0970e67a","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"collapsed":true,"cell_id":"00001-39e3901a-8248-46b3-8451-16a83ea8160e","deepnote_cell_type":"code"},"source":"# Import\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Main Processing Cell\n\nPerform following operations:\n\n__A__: Declare a simple model consisting of Input Layer, followed by 1 Dense Layer with single neuron. Perform forward pass for the data example __xIn__ of batch size = 10 and feature vector size = 3. Plot the results. Repeat the process using all 3 activation functions.\n\n__B__: Declare a simple model consisting of Input Layer, followed by 3 Dense Layers of arbitrary size and 1 Dense Layer with 2 neurons. Declare your own input data with batch size 16 and vector size of 10. Perform forward pass through the network and visualize activations of each resulting neuron.","metadata":{"cell_id":"00002-31fcceef-32f2-4a76-900b-ea59f8ce91cd","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"collapsed":true,"cell_id":"00003-720daf1e-4402-4599-bac8-7683d13f74c7","deepnote_cell_type":"code"},"source":"xIn = np.array([[-4, -4, -4],\n               [-3, -3, -3],\n               [-2, -2, -2],\n               [-1, -1, -1],\n               [-0, -0, -0],\n               [1, 1, 1],\n               [2, 2, 2],\n               [3, 3, 3],\n               [4, 4, 4],\n               [5, 5, 5]])\n###>>> start of solution\nm = Model()\nm.addLayer(InputLayer(3)) # number of input features per data\nm.addLayer(DenseLayer(1, act=\"linear\"))\nm.initialize()\n\ny = m.predict(xIn)\n###<<< end of solution\nplt.plot(y)\nplt.show()\n\n###>>> start of solution\nm = Model()\nm.addLayer(InputLayer(3))\nm.addLayer(DenseLayer(1, act=\"sigmoid\"))\nm.initialize()\n\ny = m.predict(xIn)\n###<<< end of solution\nplt.plot(y)\nplt.show()\n\n\n###>>> start of solution\nm = Model()\nm.addLayer(InputLayer(3))\nm.addLayer(DenseLayer(1, act=\"relu\"))\nm.initialize()\n\ny = m.predict(xIn)\n###<<< end of solution\nplt.plot(y)\nplt.show()\n\n\n\n# Task B:\nxIn = np.random.randn(16, 10)\n###>>> start of solution\nm = Model()\nm.addLayer(InputLayer(10))\nm.addLayer(DenseLayer(10, act=\"sigmoid\"))\nm.addLayer(DenseLayer(5, act=\"sigmoid\"))\nm.addLayer(DenseLayer(2, act=\"sigmoid\"))\nm.initialize()\n\ny = m.predict(xIn)\n###<<< end of solution\ny = np.transpose(y) # predictions are per data for each neuron, we want to plot all activations from first and second neuron to compare\nplt.plot(y[0])\nplt.plot(y[1])\nplt.show()\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Layer\n\nThis is the main class which can hold different types of layers and provide us with standard tasks like forward propagation. Perform the variable declaration, provide your solution with variable initialization when later called by your model and also functionality to perform forward pass. We added another class of Neuron to the Dense Layer for simplicity. You can perform the logic of a layer per single neuron using lists and for cycles. You can declare activation function in your __init__ function of the parent class (Layer) and call it later when you perform forward pass, or perform it in one of the child classes.\n\nnUnits - number of neuron units in your layer\n\nprevLayer - previous layer (need it to know the shape of it to create appropriate number of weights for you to use in current layer)\n\nnX - number of units (in case of input layer number of features for each data vector)","metadata":{"cell_id":"00004-9f7acf15-5a66-4c8d-9df0-f73f6ff34f6c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00005-4c24033d-43f4-4960-97b1-bccf77294208","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"(, 1)","metadata":{"tags":[],"cell_id":"00006-1a836390-99dd-43b0-a83c-d13dc7cb6f67","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"collapsed":true,"cell_id":"00006-b1e0140a-d9d0-4cc7-a027-a88151cccc84","deepnote_cell_type":"code"},"source":"#------------------------------------------------------------------------------\n#   Layer class\n#------------------------------------------------------------------------------\nclass Layer:\n    def __init__(self, act=\"linear\"):\n        self.shape = (0, 0)\n        #TODO: declare mapping of your activation function here (or later in child class if you like)\n        act_functions = {\n            'linear': LinearActivationFunction,\n            'sigmoid': SigmoidActivationFunction,\n            'relu': RELUActivationFunction\n        }\n        ###>>> start of solution\n        self.activation = act_functions[act]\n        ###<<< end of solution\n        pass\n\n    def initialize(self, prevLayer):\n        pass\n\n    def forward(self, x, isTraining):\n        pass\n\n#------------------------------------------------------------------------------\n#   InputLayer class\n#------------------------------------------------------------------------------\nclass InputLayer(Layer):\n    def __init__(self, nX):\n        super().__init__(act=\"linear\")\n\n        self.nX = nX\n\n    def initialize(self, prevLayer):\n        self.shape = (self.nX, 1)\n\n    def forward(self, x):\n        return x\n    \n#------------------------------------------------------------------------------\n#   Basic Dense Layer class\n#------------------------------------------------------------------------------\nclass DenseLayer(Layer):\n    def __init__(self, nUnits, act=\"linear\"):\n        super().__init__(act)\n        ###>>> start of solution\n        self.size = (, nUnits)\n        self.neurons = []\n        ###<<< end of solution\n        pass\n\n    def initialize(self, prevLayer):\n        ###>>> start of solution\n        #initialize all neurons\n        for i, _ in range(self.size):\n            self.neurons.append(Neuron(self.activation))\n        ###<<< end of solution\n        pass\n\n    def forward(self, X):\n        ###>>> start of solution\n        return np.array([n.forward(X) for n in self.neurons])\n        ###<<< end of solution\n        pass\n    \n    class Neuron:\n        \"\"\"Single Neuron used in the Dense Layer\"\"\"\n        def __init__(self, act=\"linear\"):\n            ###>>> start of solution\n            self.w = np.uniform(0., 1.)\n            self.b = 0\n            ###<<< end of solution\n            \n        def initialize(self, prevLayer):\n            ###>>> start of solution\n            \n            ###<<< end of solution\n            pass\n\n        def forward(self, X):\n            ###>>> start of solution\n            return act(np.sum(w * X + b))\n            ###<<< end of solution\n","execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Activations\n\nImplement three standard activation functions (Linear, ReLU, Sigmoid), which you will use in your implementation of dense layer for logical neurons.","metadata":{"cell_id":"00007-c1576bf8-b14e-400c-9b25-4f44cabe08db","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"collapsed":true,"cell_id":"00008-7b21ec22-738e-479d-81f6-1a604cb1cd0b","deepnote_cell_type":"code"},"source":"#------------------------------------------------------------------------------\n#   ActivationFunction class\n#------------------------------------------------------------------------------\nclass ActivationFunction:\n    def __init__(self):\n        pass\n\n    def __call__(self, Z):\n        pass\n\n#------------------------------------------------------------------------------\n#   LinearActivationFunction class\n#------------------------------------------------------------------------------\nclass LinearActivationFunction(ActivationFunction):\n    def __call__(self, Z):\n        ###>>> start of solution\n        return Z\n        ###<<< end of solution\n        pass\n\n    def derivate(self, Z):\n        # do not need to implement now\n        pass\n    \n#------------------------------------------------------------------------------\n#   SigmoidActivationFunction class\n#------------------------------------------------------------------------------\nclass SigmoidActivationFunction(ActivationFunction):\n    def __call__(self, Z):\n        ###>>> start of solution\n        return 1 / (1 + np.exp(-Z))\n        ###<<< end of solution\n        pass\n\n    def derivate(self, Z):\n        # do not need to implement now\n        pass\n\n#------------------------------------------------------------------------------\n#   RELUActivationFunction class\n#------------------------------------------------------------------------------\nclass RELUActivationFunction(ActivationFunction):\n    def __call__(self, Z):\n        ###>>> start of solution\n        return np.where(Z > 0, Z, 0)\n        ###<<< end of solution\n        pass\n\n    def derivate(self, Z):\n        # do not need to implement now\n        pass\n    \n# Activation mapping\n    \nMAP_ACTIVATION_FUCTIONS = {\n    \"linear\": LinearActivationFunction,\n    \"relu\": RELUActivationFunction,\n    \"sigmoid\": SigmoidActivationFunction\n}\n\ndef CreateActivationFunction(kind):\n    if (kind in MAP_ACTIVATION_FUCTIONS):\n        return MAP_ACTIVATION_FUCTIONS[kind]()\n    raise ValueError(kind, \"Unknown activation function {0}\".format(kind))","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Model class\n\nThis is the basic class which holds all of your layers and encapsulate functionality to predict results from your input as a forward pass through all the layers after you create your model and initialize all the layers.","metadata":{"cell_id":"00009-e21d472c-f20d-49fa-8581-d4deff0c7a83","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"collapsed":true,"cell_id":"00010-60e4a53d-5998-4388-a1fc-feae24a33059","deepnote_cell_type":"code"},"source":"#------------------------------------------------------------------------------\n#   Model class\n#------------------------------------------------------------------------------\nclass Model:\n    def __init__(self):\n        self.layers = []\n\n    def addLayer(self, layer):\n        self.layers.append(layer)\n\n    def initialize(self):\n        ###>>> start of solution\n        self.layers[0].initialize(None)\n        for i in range(1, len(self.layers)):\n            self.layers[i].initialize(self.layers[i-1].shape)\n        ###<<< end of solution\n        pass\n    \n    def predict(self, X):\n        ###>>> start of solution\n        \n        ###<<< end of solution\n        pass","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"cell_id":"00011-985f77e3-8241-4f61-b706-1231d76485a8","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=151ec30a-9db5-4600-8a2d-358dada1a208' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"deepnote_notebook_id":"74ab91bc-2a31-48e7-9b79-d59928a27829","deepnote":{},"deepnote_execution_queue":[]}}